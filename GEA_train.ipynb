{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1755996-aa53-4a4b-88c2-fea3027b5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bab3bec-0de6-41e0-9ddd-d2059e75e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Instance Of Deprecation Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "096f5438-21f2-4828-8568-38399970d9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-pandas\n",
      "  Downloading sklearn_pandas-2.2.0-py2.py3-none-any.whl.metadata (445 bytes)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from sklearn-pandas) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.5.1 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from sklearn-pandas) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from sklearn-pandas) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from sklearn-pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn-pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn-pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn-pandas) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn-pandas) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn-pandas) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dennis\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->sklearn-pandas) (1.16.0)\n",
      "Downloading sklearn_pandas-2.2.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sklearn-pandas\n",
      "Successfully installed sklearn-pandas-2.2.0\n"
     ]
    }
   ],
   "source": [
    "# Install Latest Version Available\n",
    "#!pip install tensorflow\n",
    "#!pip install holidays\n",
    "!pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df80069e-6d73-4923-bb50-f0e9f5722bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4adb6c36-2108-42fd-9edc-903a0c000499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler \n",
    "from sklearn.compose import ColumnTransformer as ct\n",
    "import os as os\n",
    "#from datetime import date, timedelta, datetime\n",
    "from joblib import dump, load\n",
    "import time\n",
    "import gc\n",
    "import holidays\n",
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338fda15-9359-4ebf-9dc0-81cadf9b25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, concatenate, Input\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import mae, mse, mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec50651a-c769-48cc-ab0b-cd0e6a1c383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7051424579943862867\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check computational platform\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db09b72e-0984-4952-95d4-9f6ce30970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "cols = ['sku','platform','customer','week_begin_date','week_end_date','price','quantity',\n",
    "        'employment','inflation','GDP']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "sdate = dt.date(2024,1,1)\n",
    "edate=dt.date(2024,12,31)\n",
    "wbd = [(sdate+timedelta(days=x)) for x in range(0,(edate-sdate).days,7)]\n",
    "wed = [(x+timedelta(days=6)) for x in wbd][:-1]\n",
    "wed.append(edate)\n",
    "df['week_end_date'] = pd.to_datetime(wed)\n",
    "df['week_begin_date'] = pd.to_datetime(wbd)\n",
    "\n",
    "#df['date'] = pd.to_datetime(df['date'])\n",
    "#df['year'], df['month'] = df['date'].dt.year, df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc0e86a-82df-46ee-8f62-f4f22933c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function To select platform from column - platform is a list of strings\n",
    "platform = []\n",
    "def select_platform(data, platform):\n",
    "  data = data[data[platform]==platform]\n",
    "  #data.drop(columns=[platform],axis=1,inplace=True)\n",
    "  data.reset_index(drop=True, inplace=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d7712e-a172-4cdf-8a05-2544ca18f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select customer from column - customer is a string\n",
    "customer = []\n",
    "def select_customer(data, customer):\n",
    "  data = data[data[customer]==customer]\n",
    "  #data.drop(columns=[customer],axis=1,inplace=True)\n",
    "  data.reset_index(drop=True, inplace=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8035169-59d1-42f4-b64a-8b32c8152318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dates to categorical variables\n",
    "# Date is the sample date column name - week_end_date here\n",
    "def add_date_features(data,date,name):\n",
    "    data[name + '_yr'] = data[date].dt.year\n",
    "    data[name + '_day'] = data[date].dt.dayofyear\n",
    "    #data[name + '_week'] = [x.isocalendar()[1] for x in data[date]]\n",
    "    data[name + '_mon'] = data[date].dt.month \n",
    "    #data[name + '_qtr'] = data[date].dt.quarter\n",
    "    #data.drop([date], axis = 1, inplace = True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc9d5f7-968f-4f2b-8993-10803e0e622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select cluster\n",
    "# [segment] is the clustered SKU's in a list\n",
    "segment = []\n",
    "def select_cluster(data, segment):\n",
    "  data = data[data['sku'].isin(segment)]\n",
    "  #data.drop(columns=['SKU'],axis=1,inplace=True)\n",
    "  data.reset_index(drop=True, inplace=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838e7ef9-907a-47e0-96ec-745fd44319c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce displayed decimals in continuous variables for readability\n",
    "def rem_decimals(data, feat):\n",
    "  data[feat] = data[feat].apply(lambda x: int(x))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff92ae57-8d51-4038-be10-05e043a18797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add black friday to holidays\n",
    "# Arg years is list of years astype int\n",
    "def add_blk_fri(years):\n",
    "    for y in years:\n",
    "        h = holidays.US(years=[y])\n",
    "        h[dt.date(y, 11, 29)] ='Black Friday'\n",
    "        #hdates = [date for date,name in h.items()]\n",
    "        #hdays = [name for date,name in h.items()]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ef9b96-3428-42ad-bb22-e28d2825f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bus_days(data):\n",
    "    begin =  pd.to_datetime(data[\"week_begin_date\"]).values.astype('datetime64[D]') \n",
    "    end = pd.to_datetime(data[\"week_end_date\"]).values.astype('datetime64[D]')\n",
    "    bus_days = [np.busday_count(begin,end)]\n",
    "    return bus_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4181d47-2a75-4076-b2a5-083bcc51fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wk_feature(data,date,name):\n",
    "    data[name + '_week'] = [x.isocalendar()[1] for x in data[date]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b8ac4-6d8d-407f-835b-969f97d90dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Functions To Prepare Data For Training\n",
    "\n",
    "# Select SKU cluster\n",
    "sku_segment = []\n",
    "data = select_cluster(df, segment) #only use df once to preserve original data in notebook\n",
    "\n",
    "# Remove decimals\n",
    "data = rem_decimals(data, 'price')\n",
    "data = rem_decimals(data, 'quantity')\n",
    "\n",
    "# Add business days per week\n",
    "data[\"bus_days\"] = add_bus_days(data)\n",
    "\n",
    "# Add date features\n",
    "data = add_date_features(data,'week_end_date', 'week_end')\n",
    "\n",
    "# Add isocalendar week features\n",
    "data = add_wk_features(data,'week_end_date', 'week_end')\n",
    "\n",
    "# Set column order\n",
    "cols = ['sku','platform','customer','week_end_date','week_end_year','week_end_mon','week_end_week','price','quantity',\n",
    "        'employment','inflation','GDP', 'week_end_day']\n",
    "data = data[cols]\n",
    "\n",
    "# Sort by time & reindex for simplicity\n",
    "data.sort_values(['week_end_date','platform','customer','sku'] ,axis=0,ascending=True,inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Save data\n",
    "filepath = ''\n",
    "dump(data, filepath + 'data_prepped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c1667-226a-4a9e-bbb2-79b676ab6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights to balance wide distributions of quantity\n",
    "# 'sku' is  column name for setting weights\n",
    "# 'quantity' is column name for continuous quantity to become weights\n",
    "# setting hi and lo will limit the adjusted relative weights of the sample weighs to prevent over emphasis of small volume sku's \n",
    "\n",
    "def compute_sample_weights(data,hi=1.2,lo=.8):\n",
    "  \n",
    "  # make list of sku\n",
    "  sku_ = [data.groupby('sku')] \n",
    "  \n",
    "  # sum quantity over aggregation of colname1\n",
    "  sku_qty = [data.groupby(['sku'])['quantity'].agg('sum')]\n",
    "  \n",
    "  # set weight tensor equal to proportion relative to total of quantity \n",
    "  total_qty = data['quantity'].sum()\n",
    "\n",
    "  # compute relative weight of each sku\n",
    "  raw_weights = [x / total_qty for x in sku_qty]\n",
    "  \n",
    "  # scale weights to equalize impact of large quantity sku's with low quantity sku's\n",
    "  # set range limits for sample weights in lo and hi\n",
    "  scaler = MinMaxScaler(feature_range = (lo,hi))\n",
    "  scaled_weights = [scaler.fit_transform(raw_weights)]\n",
    "  \n",
    "  \n",
    "  reverse_scaled_weights = scaled_weights.reverse()\n",
    "  # create dictionary of sku and scaled_weights\n",
    "  sku_weights = zip(sku_,reverse_scaled_weights)\n",
    "  \n",
    "  return sku_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef05d-e0f5-4974-9d1d-c06917aea7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate data windows for training, validaton and test as in a time series\n",
    "# training_width, target_width and shift are in weeks\n",
    "# training_width is training data in weeks\n",
    "# target_width is prediction window in weeks\n",
    "# shift is gap between training data and target data\n",
    "\n",
    "\n",
    "def training_window_generator(training_width, target_width, shift, data):\n",
    "    \n",
    "    # compute total window size in weeks\n",
    "    window_size = (training_width + shift + target_width)  \n",
    "\n",
    "    # compute first date of training period\n",
    "    begin_training = (data['week_end_date'].max() - timedelta(weeks = window_size))\n",
    "\n",
    "    # compute last date of training period\n",
    "    end_training = begin_training + timedelta(weeks = training_width)\n",
    "\n",
    "    # compute first date of target period\n",
    "    begin_target = end_training + timedelta(weeks = shift)\n",
    "\n",
    "    # compute end date of target period\n",
    "    end_target = begin_target + timedelta(weeks = target_width)\n",
    "\n",
    "    return begin_training, end_training, begin_target, end_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f878c-b087-4570-aa63-b4c0cbbb0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "training_data = data.loc[(data['week_end_date' >= begin_training) & (data['week_end_date'] <= end_training)]\n",
    "validation_data = data.loc[(data['week_end_date' >= begin_target) & (data['week_end_date'] <= end_target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f0547-c50b-4f22-b93b-d2995dd80bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Independent Variables By Data Type\n",
    "\n",
    "# Choose Categorical Vars\n",
    "cat_vars = ['sku', 'customer', 'platform']\n",
    "\n",
    "# Choose Time Features Vars\n",
    "time_vars = ['week_end_year','week_end_mon','week_end_week']\n",
    "\n",
    "# Choose Continuous Feature Vars To Be Scaled\n",
    "cont_vars = ['price','quantity','unemploment','inflation','gdp']\n",
    "\n",
    "# Choose Continuous Feature Vars To Be Logged\n",
    "cont_vars_log = []\n",
    "\n",
    "# Include control features\n",
    "control_vars =['week_end_date']\n",
    "\n",
    "# Append features \n",
    "features = cat_vars + time_vars + cont_vars + cont_vars_log + control_vars\n",
    "\n",
    "# Create data from features list\n",
    "data = data[features]\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b934a8-5305-4b21-bcdf-c776c40f40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take log on continuous features to be logged with +1 to filter negative logs\n",
    "\n",
    "def log_cont_data(data, cont_vars_log=cont_vars_log):   \n",
    "  # take logs of each variable to be logged identified\n",
    "    if len(cont_vars_log) > 0:\n",
    "        for feat in cont_vars_log:\n",
    "            data[feat] = data[feat].apply(lambda x: np.log1p(x).astype(np.float32))\n",
    "    elif\n",
    "        pass\n",
    "  # return data with logged columns\n",
    "  data.reset_index(drop=True, inplace=True)\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f13653-db93-46a3-b116-6aa23e55d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute len of embeddings for categorical features in data\n",
    "\n",
    "def cat_map_data(data,cat_vars=cat_vars, emax=25, emin=4):\n",
    "    # compute list of number of unique categories for each categorical variable\n",
    "    cat_emb = [len(data[c].unique()) for c in cat_vars]\n",
    "    \n",
    "    # compute list inserting maximum number of embeddings for each category \n",
    "    cat_emb_max = [c if c<= emax else emax for c in cat_emb] #maximum embedded weights is emax (default=50)\n",
    "    \n",
    "    # compute list inserting minimum number of embeddings for each category\n",
    "    cat_emb_max = [c if c>= emin else emin for c in cat_emb_max] #minimum embedded weights is emin (default = 4)\n",
    "    \n",
    "    # form dictionary of the categorical variables and the list of embeddings\n",
    "    cat_vars_dict = dict(zip(cat_vars,cat_emb_max))\n",
    "    \n",
    "    # form list of tuples of categorical variables and the label encoder\n",
    "    cat_map = [(c,LabelEncoder()) for c in cat_vars]\n",
    "    \n",
    "    # return the embedding dictionary and the map of label encoders to categorical variables\n",
    "    return cat_vars_dict,cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442d9ea-d3f1-4cad-a171-b524e7531bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply embeddings for time features in data\n",
    "\n",
    "def time_map_data(data,time_vars=time_vars, tmax=12, tmin=3):\n",
    "    \n",
    "    # compute number of unique values for each time variable\n",
    "    time_emb = [len(data[t].unique()) for t in time_vars]\n",
    "    \n",
    "    # insert maximum embedded coefficients for time variables\n",
    "    time_emb_max = [t if t <= tmax else tmax for t in time_emb] #maximum embedded weights is tmax (default=12)\n",
    "    \n",
    "    # insert minimum embedded coefficients for time variables\n",
    "    time_emb_max = [t if t >= tmin else tmin for t in time_emb_max]#minimum embedded weights is tmin (default=3)\n",
    "    time_vars_dict = dict(zip(time_vars,time_emb_max))\n",
    "    \n",
    "    # compute list of tuples assigning the Label Encoder to the time variable\n",
    "    time_map = [(t,LabelEncoder()) for t in time_vars]\n",
    "    \n",
    "    # return dictionary of embedded coefficients and list of encoder tuples\n",
    "    return time_vars_dict,time_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81585f6e-aa3b-4a82-8576-29772df891e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply scaler on the continuous features in data\n",
    "\n",
    "# s can be standardscaler,robustscaler or minmaxscaler; default is minmax\n",
    "# x,y is limit on minmax; default to 0,1\n",
    "# l,u is percential rank for the robust scaler based on median; default is 10,90\n",
    "\n",
    "def cont_map_data(cont_vars=cont_vars, s='minmax', x=1, y=2, l=10, u=90): # s can be standardscaler,robustscaler or minmaxscaler\n",
    "    # select scaler map and form list of tuples for variable and scaler\n",
    "  if s == 'standard':\n",
    "      cont_map = [([c],StandardScaler(copy=True,with_mean=True,with_std=True)) for c in cont_vars]\n",
    "\n",
    "  elif s == 'robust':\n",
    "      cont_map = [([c],RobustScaler(with_centering=True,with_scaling=True,quantile_range=(t,u))) for c in cont_vars]\n",
    "\n",
    "  elif s == 'minmax':\n",
    "      cont_map = [([c],MinMaxScaler(feature_range = (x,y))) for c in cont_vars]\n",
    "\n",
    "  # return map of scaler and continuous variables tuples\n",
    "  return cont_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfe131-e388-4154-8356-15e1c52588bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous features and fit with DataFrameMapper\n",
    "\n",
    "def map_cont_data(data):\n",
    "  \n",
    "  # map scaler to continuous data;  cont_map_data defaults to cont_vars and minmax scaler with x = 1,y = 2\n",
    "  cont_map = cont_map_data()\n",
    "  \n",
    "  # intialize DataFrameMapper with scalers to be applied\n",
    "  cont_mapper = DataFrameMapper(cont_map)\n",
    "\n",
    "  # fit mapper to data; cont_map_fit is required when building input layers\n",
    "  cont_map_fit = cont_mapper.fit(data)\n",
    "\n",
    "  # save scaler for prediction algorithm\n",
    "  filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'cont_scaler'\n",
    "  dump(cont_map,filename)\n",
    "  \n",
    "  # transform and return data\n",
    "  cont_data = cont_map_fit.transform(data).astype(np.float32)\n",
    "\n",
    "  return cont_data, cont_map_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c566e-438e-4c43-a542-08ac2e130dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features and fit - transform with DataFrameMapper\n",
    "\n",
    "def map_cat_data(data):\n",
    "    \n",
    "    # map encoder to categorical variables; cat_vars_dict required for input layers\n",
    "    cat_vars_dict,cat_map = cat_map_data(data)\n",
    "    \n",
    "    # save cat map for predictor\n",
    "    filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'cat_map'\n",
    "    dump(cat_map,filename)\n",
    "\n",
    "    # initialize dataframe mapper\n",
    "    cat_mapper = DataFrameMapper(cat_map)\n",
    "    \n",
    "    # fit categorical variables; cat_map_fit required for input layer\n",
    "    cat_map_fit = cat_mapper.fit(data)\n",
    "    \n",
    "    # transform data using dataframe mapper\n",
    "    cat_data = cat_map_fit.transform(data).astype(np.int64) \n",
    "    \n",
    "    # return cat_vars_dict and cat_map_fit used in embedding input layer to tensorflow\n",
    "    return  cat_data, cat_vars_dict, cat_map_fit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370031ad-f203-4842-a00f-23f337497cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode time features and fit - transform with DataFrameMapper\n",
    "  \n",
    "def map_time_data(data):    \n",
    "    \n",
    "  # map encoder to time variables; time_vars_dict used in embedding input layer to tensorflow\n",
    "  time_vars_dict,time_map = time_map_data(data)\n",
    "\n",
    "  # save time map for predictor\n",
    "  filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'time_map'\n",
    "  dump(time_map,filename)\n",
    "\n",
    "  # initialize dataframemapper\n",
    "  time_mapper = DataFrameMapper(time_map)\n",
    "\n",
    "  # fit time variables to data; time_map_fit req'd for input layers\n",
    "  time_map_fit = time_mapper.fit(data)\n",
    "\n",
    "  # transform data\n",
    "  time_data = time_map_fit.transform(data).astype(np.int64)\n",
    "\n",
    "  # return encoded time data, time_vars_dict and time_map_fit used in input layer\n",
    "  return time_data, time_vars_dict, time_map_fit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480353d2-617b-43ae-aef3-a253d1694df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous features and fit with DataFrameMapper\n",
    "\n",
    "def map_cont_data(data):\n",
    "  \n",
    "  # map scaler to continuous data;  cont_map_data defaults to cont_vars and minmax scaler with x = 1,y = 2\n",
    "  cont_map = cont_map_data()\n",
    "  \n",
    "  # intialize DataFrameMapper with scalers to be applied\n",
    "  cont_mapper = DataFrameMapper(cont_map)\n",
    "\n",
    "  # fit mapper to data; cont_map_fit is required when building input layers\n",
    "  cont_map_fit = cont_mapper.fit(data)\n",
    "\n",
    "  # save scaler for prediction algorithm\n",
    "  filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'cont_scaler'\n",
    "  dump(cont_map,filename)\n",
    "  \n",
    "  # transform and return data\n",
    "  cont_data = cont_map_fit.transform(data).astype(np.float32)\n",
    "\n",
    "  return cont_data, cont_map_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22836acc-2c75-453b-a1b0-3eef34c7ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale and shape continuous variable target tensor\n",
    "# \n",
    "def map_cont_target(data, target, s='minmax', f=1, c=2, l=10, u=90):     \n",
    "\n",
    "    # set series to single vector\n",
    "    y = data[target].reshape(-1,1)\n",
    "    \n",
    "    # select scaler    \n",
    "    if s == 'standard':\n",
    "        scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "        # fit scaler to column\n",
    "        scaled = scaler.fit(y)\n",
    "\n",
    "        # save scaler to quantity_maps subdirectory  \n",
    "        filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'target_cont_scaler' \n",
    "        dump(scaled,filename)\n",
    "\n",
    "        # transform target\n",
    "        y_scaled = scaled.transform(y)\n",
    "    \n",
    "    elif s == 'robust':\n",
    "        scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(l,u))\n",
    "\n",
    "        # fit scaler to target\n",
    "        scaled = scaler.fit(y)\n",
    "\n",
    "        # save scaler to quantity_maps subdirectory   \n",
    "        filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'target_cont_scaler' \n",
    "        #filename = os.path.join('quantity_maps','target_cont_scaler')\n",
    "        dump(scaled,filename)\n",
    "\n",
    "        # transform target\n",
    "        y_scaled = scaled.transform(y)\n",
    "\n",
    "    # note:applying minmax scaler to log removes need for leakyRelu in model\n",
    "    elif s == 'minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(f,c))\n",
    "\n",
    "        # fit scaler to column\n",
    "        scaled = scaler.fit(y)\n",
    "\n",
    "        # save scaler to quantity_maps subdirectory\n",
    "        filename = '/dbfs/CA_Predictor/' + 'prime_maps/' + 'target_cont_scaler'\n",
    "        dump(scaled,filename)\n",
    "\n",
    "        # transform target\n",
    "        y_scaled = scaled.transform(y)\n",
    "    \n",
    "    # shape and type y array for tensorflow\n",
    "    data[feat] = y_scaled.astype(np.float32) \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b80411-1534-44c3-856d-245ecaf9468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transformed and shaped input data\n",
    "\n",
    "# scale and encode data \n",
    "\n",
    "# # encode categorical variables\n",
    "data_cat, cat_vars_dict, cat_map_fit = map_cat_data(data)\n",
    "\n",
    "# encode time variables\n",
    "data_time, time_vars_dict, time_map_fit = map_time_data(data)\n",
    "\n",
    "# scale selected continuous variables\n",
    "data_cont, cont_map_fit = map_cont_data(data)\n",
    "\n",
    "# log selected continuous variables\n",
    "data_log = log_cont_data(data)\n",
    "\n",
    "# # scale or log target\n",
    "data_target = log_target(data[target]) #log target data\n",
    "# data_target = map_cont_target(data[target]) #scale target data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa9c7b-4622-4e89-b90d-8ff97a1c870e",
   "metadata": {},
   "outputs": [],
   "source": [
    ",Stratify and perform train validate test split & shape for tensor\n",
    "\n",
    "# Compute target, data, stratification array and percentile boundaries\n",
    "y = data_scaled[:,-1].reshape(-1,)\n",
    "X = data_scaled[:,:-1]\n",
    "\n",
    "# Compute stratifications\n",
    "strats, percentiles = pd.qcut(y.reshape(-1,),4,labels=False,retbins=True)\n",
    "y = np.column_stack((y,strats))\n",
    "\n",
    "# Perform first split to obtain stratified all and test set \n",
    "X_all,X_test,y_all,y_test = train_test_split(X,y,test_size=.05,random_state=75,stratify=strats)\n",
    "\n",
    "# Redefine strats to be stratification in remaining target vector y_all; remove strats from y_all\n",
    "strats = y_all[:,1]\n",
    "y_all = y_all[:,0]\n",
    "\n",
    "# Remove strats from y_test\n",
    "y_test = y_test[:,0]\n",
    "\n",
    "# # Perform second split to obtain stratified train and validation sets \n",
    "X_train,X_val,y_train,y_val = train_test_split(X_all,y_all,test_size=.15,random_state=75,stratify=strats)\n",
    "\n",
    "# # Extract sample weights from X\n",
    "test_weights = X_test[:,-1] \n",
    "train_weights = X_train[:,-1]\n",
    "val_weights = X_val[:,-1]\n",
    "all_weights = X_all[:,-1]\n",
    "\n",
    "# Remove sample weights from data\n",
    "X_train = X_train[:,:-1]\n",
    "X_all = X_all[:,:-1]\n",
    "X_val = X_val[:,:-1]\n",
    "X_test = X_test[:,:-1]\n",
    "\n",
    "# Convert train and test input data to list of arrays for tensorflow\n",
    "X_tr = np.hsplit(X_train, X_train.shape[1])\n",
    "X_va = np.hsplit(X_val, X_val.shape[1])\n",
    "X_te = np.hsplit(X_test, X_test.shape[1])\n",
    "X_al = np.hsplit(X_all, X_all.shape[1])\n",
    "\n",
    "# Shape targets\n",
    "y_tr = np.array(y_train).reshape(-1,1)\n",
    "y_va = np.array(y_val).reshape(-1,1)\n",
    "y_te =  np.array(y_test).reshape(-1,1)\n",
    "y_al = np.array(y_all).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0395097-d9b3-42de-b8dd-8ab84524b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "Form categorical variable input layer with linear first stage embedding\n",
    "\n",
    "# Apply batch normalization for both regularization and controlling gradients\n",
    "# Apply dropout to control overfitting and exploding/vanishing gradients\n",
    "\n",
    "def cat_input(feat,cat_vars_dict,r=.5):\n",
    "    # compute input vector\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = cat_vars_dict[name]\n",
    "\n",
    "    # create input layer\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    cat = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    \n",
    "    # add dense layers, dropout, and batch normalization\n",
    "    cat = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cat)\n",
    "    cat = Dropout(rate=r)(cat)\n",
    "    cat = BatchNormalization()(cat)\n",
    "    cat = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cat)\n",
    "    cat = Dropout(rate=r)(cat)\n",
    "    cat = BatchNormalization()(cat)\n",
    "    return inp,cat\n",
    "\n",
    "# Graph categorical features input\n",
    "cats = [cat_input(feat,cat_vars_dict) for feat in cat_map_fit.features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783f7ce-cf91-4d17-b0fa-8c221c76d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Form time variable input layer with linear first stage embedding\n",
    "\n",
    "\n",
    "def time_input(feat,time_vars_dict,r=.5):\n",
    "    \n",
    "    # compute input vector\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = time_vars_dict[name]\n",
    "\n",
    "    # create input layer\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    time = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    \n",
    "    # add dense, dropout and normalization layers\n",
    "    time = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(time)\n",
    "    time = Dropout(rate=r)(time)\n",
    "    time = BatchNormalization()(time)\n",
    "    time = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(time)\n",
    "    time = Dropout(rate=r)(time)\n",
    "    time = BatchNormalization()(time)\n",
    "    return inp,time\n",
    "\n",
    "# Graph time features input\n",
    "times = [time_input(feat,time_vars_dict) for feat in time_map_fit.features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9b63c-4066-4c2d-a908-72e058bfb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Form continuous variable input layer with linear first stage embedding\n",
    "\n",
    "def cont_input(feat,r=.5):\n",
    "    name = feat[0][0]\n",
    "    \n",
    "    # create input layer\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    cont = Dense(1, name = name + '_d')(inp)\n",
    "\n",
    "    # add dense, dropout, batch normalization layers\n",
    "    cont = Dense(1000, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cont)\n",
    "    cont = Dropout(rate=r)(cont)\n",
    "    cont = BatchNormalization()(cont)\n",
    "    cont = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cont)\n",
    "    cont = Dropout(rate=r)(cont)\n",
    "    cont = BatchNormalization()(cont)\n",
    "    return inp,cont\n",
    "\n",
    "# Graph continuous features input\n",
    "conts = [cont_input(feat) for feat in cont_map_fit.features] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef8e03-3ef3-4c83-bb7f-ce324dbc4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Form continuous logged variable input layer with linear first stage embedding\n",
    "def log_cont_input(feat,r=.5):\n",
    "    name = str(feat)\n",
    "    \n",
    "    # create input layer\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    cont = Dense(1, name = name + '_d')(inp)\n",
    "\n",
    "    # add dense, dropout, batch normalization layers\n",
    "    cont = Dense(1000, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cont)\n",
    "    cont = Dropout(rate=r)(cont)\n",
    "    cont = BatchNormalization()(cont)\n",
    "    cont = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cont)\n",
    "    cont = Dropout(rate=r)(cont)\n",
    "    cont = BatchNormalization()(cont)\n",
    "    return inp,cont\n",
    "\n",
    "# Graph continuous features input\n",
    "log_conts = [log_cont_input(feat) for feat in data_log.columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b02ad-6bb4-47e9-ada6-d58673659eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,For interconnected layers among and between variables\n",
    "def build_quantity_model(cats, times, conts, log_conts, r = .5):\n",
    "\n",
    "    # Build graph for interconnected categorical features\n",
    "    \n",
    "    # input concatenated categorical features for interconnected nodes\n",
    "    c = concatenate([cat for inp,cat in cats], axis=1)\n",
    "    \n",
    "    # add dense, dropout and batch normalization layers\n",
    "    c = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(c)\n",
    "    c = Dropout(rate=r)(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(c)\n",
    "    c = Dropout(rate=r)(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    \n",
    "    # Build graph for concatenated time features for interconnected nodes\n",
    "    \n",
    "    # concatenate time variables for inteconnected nodes\n",
    "    t = concatenate([time for inp,time in times], axis = 1)\n",
    "    \n",
    "    # add dense, dropout and batch normalization layers \n",
    "    t = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(t)\n",
    "    t = Dropout(rate=r)(t)\n",
    "    t = BatchNormalization()(t)\n",
    "    t = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(t)\n",
    "    t = Dropout(rate=r)(t)\n",
    "    t = BatchNormalization()(t)\n",
    "\n",
    "    # Build graph for continuous features fully interconnected by concatenate\n",
    "    \n",
    "    # concatenate if 2 or more variables\n",
    "    f = concatenate([cont for inp,cont in conts], axis=1)\n",
    "    \n",
    "    # add dense, dropout and batch normalization layers fully interconnected\n",
    "    f = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(f)\n",
    "    f = Dropout(rate=r)(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(f)\n",
    "    f = Dropout(rate=r)(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    \n",
    "     # Build graph for logged continuous features fully interconnected by concatenate\n",
    "    \n",
    "    # concatenate if 2 or more variables\n",
    "    l = concatenate([cont for inp,cont in log_conts], axis = 1)\n",
    "\n",
    "    # add dense, dropout and batch normalization layers fully interconnected\n",
    "    l = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(l)\n",
    "    l = Dropout(rate=r)(l)\n",
    "    l = BatchNormalization()(l)\n",
    "    l = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(l)\n",
    "    l = Dropout(rate=r)(l)\n",
    "    l = BatchNormalization()(l)\n",
    "    \n",
    "    # Concatenate categorical, time, continuous and continuous logged features\n",
    "    x = concatenate([c,t,f,l], axis=1)\n",
    "\n",
    "    # add fully interconnected dense, dropout and normalization layers\n",
    "    x = Dense(500,activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(x)\n",
    "    x = Dropout(rate=r)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(x)\n",
    "    x = Dropout(rate=r)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(x) \n",
    "    \n",
    "    # add input layer for the model\n",
    "    model = Model([inp for inp,cat in cats] + [inp for inp,time in times] + [inp for inp,cont in conts] + [inp for inp,cont in log_conts], x)\n",
    "    \n",
    "    # set learning rate\n",
    "    #adam = keras.optimizers.Adam(learning_rate=.001)\n",
    "    \n",
    "    #  compile with optimizer, loss and metrics\n",
    "    model.compile(optimizer='Adam', loss = MSE, metrics = ['mae','mse','mape'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7d892-a932-4b48-a90b-f7bfeeb8797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Call formation of the basic quantity model\n",
    "quantity_model = build_quantity_model(cats, times, conts, log_conts)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Initialize callbacks for validation\n",
    "es = EarlyStopping(monitor='loss', patience=5, verbose=0,\n",
    "    mode='min', min_delta=.015, restore_best_weights=True)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.9,\n",
    "                              mode = 'min', min_delta=.01, patience=3, min_lr=0.0005)\n",
    "\n",
    "filepath = '/dbfs/CA_Predictor/' + 'prime_val_checkpoint/' + 'val_checkpoint'\n",
    "mckp = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# #load weights from the prior model\n",
    "# # load weights from previous model\n",
    "# #filepath = '/dbfs/CA_Predictor/' + 'prime_val_model/' + 'val_weights'\n",
    "# filepath = '/dbfs/Saksham/CA_Predictor/prime_val_model/val_weights'\n",
    "# quantity_model.load_weights(filepath)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Perform validation model\n",
    "# Train the model using early stopping and reducing learning rates to reduce overfitting and 'memorization'.\n",
    "\n",
    "quantity_model.fit(X_tr,y_tr,\n",
    "            batch_size=512,\n",
    "            epochs=20,\n",
    "            verbose=True, sample_weight=train_weights,\n",
    "            validation_data = (X_va, y_va),callbacks=[es,rlr,mckp])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Call epoch history in validation model\n",
    "# call history\n",
    "dh = pd.DataFrame(data=quantity_model.history.history)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Interpret initial epoch progress\n",
    "dh.head()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Interpret last epoch performance\n",
    "dh.tail()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Compare training and validation losses\n",
    "dh[['loss','val_loss']].plot()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Compare absolute error between training and validation set\n",
    "dh[['mae','val_mae']].plot()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Compare squared error between training and validation epochs\n",
    "dh[['mse','val_mse']].plot()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "# Confirm performance using test data\n",
    "\n",
    "# make prediction in scaled log of quantity \n",
    "y_pred_test = quantity_model.predict(X_te)\n",
    "\n",
    "# compute array of differences between predictions and actuals\n",
    "y_diff_test = abs(y_te - y_pred_test)\n",
    "\n",
    "# compute percentage errors on y_test\n",
    "y_percent_error_test = 100 * y_diff_test/y_te\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# compute mean of percent error\n",
    "y_percent_error_test.mean()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# compute deviation of percent error\n",
    "y_percent_error_test.std()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# show scatter plot of error\n",
    "plt.scatter(y_te,y_pred_test)\n",
    "plt.plot(y_te,y_te,'r')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ## Saving the validation model\n",
    "\n",
    "# from tensorflow.keras.applications import InceptionV3\n",
    "# quantity_model.save('/tmp/validation_quantity_model.h5')\n",
    "\n",
    "# ## copy data to DBFS as dbfs:/tmp/model-full.h5 and check it:\n",
    "# dbutils.fs.cp(\"file:/tmp/validation_quantity_model.h5\", \"dbfs:/CA_Predictor/prime_val_model/val_model/quantity_model.h5\")\n",
    "# #display(dbutils.fs.ls(\"file:/tmp/quantity_model.h5\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Set callbacks for full model training\n",
    "# Train the predictor using all of the data set except the reserved test set for final evaluation\n",
    "\n",
    "# initiate callbacks\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=3, verbose=0,\n",
    "    mode='min', min_delta=.001, restore_best_weights=True)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.95,\n",
    "                              mode = 'min', min_delta=.0005, patience=2, min_lr=0.0005)\n",
    "\n",
    "filepath = '/dbfs/CA_Predictor/' + 'prime_all_checkpoint/' + 'all_checkpoint'\n",
    "mckp = ModelCheckpoint(filepath=filepath, monitor='loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Load validation weights for transfer learning\n",
    "#load weights from the prior model\n",
    "# load weights from previous model\n",
    "#filepath = '/dbfs/CA_Predictor/' + 'prime_val_model/' + 'val_weights'\n",
    "filepath = '/dbfs/CA_Predictor/prime_all_model/all_weights'\n",
    "quantity_model.load_weights(filepath)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Form training epochs on final fully weighted data\n",
    "# Additional training on merged train and validation set with epochs limited to the min loss epoch on validation set\n",
    "# when order_model is fit the second time, even with changed data, it loads the final weights from the first fit automatically\n",
    "# clear_session does not reset the weights in the model and the learning rate starts at the last rlr learning rate\n",
    "\n",
    "quantity_model.fit(X_al,y_al,\n",
    "            batch_size=512,\n",
    "            epochs=10,\n",
    "            verbose=True,\n",
    "            validation_data = None, \n",
    "            sample_weight=all_weights,\n",
    "            callbacks=[es,rlr,mckp])\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Save coefficient weights for predictor model\n",
    "# save coefficient weights\n",
    "filepath = '/dbfs/CA_Predictor/' + 'prime_all_model/' + 'all_weights'\n",
    "quantity_model.save_weights(filepath, overwrite=True, save_format=None, options=None)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Reload epoch history to dataframe\n",
    "\n",
    "da = pd.DataFrame(data=quantity_model.history.history)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Review first five epochs\n",
    "da.head()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Review last five epochs\n",
    "da.tail()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Confirm performance using test data\n",
    "\n",
    "# make prediction in scaled log of quantity \n",
    "y_pred_test = quantity_model.predict(X_te)\n",
    "\n",
    "# compute array of differences between predictions and actuals\n",
    "y_diff_test = y_te - y_pred_test\n",
    "\n",
    "# compute percentage errors on y_test\n",
    "y_percent_error_test = 100 * y_diff_test/y_te\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Compute mean of test percent error\n",
    "# \n",
    "y_percent_error_test.mean()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Compute deviation of percent error\n",
    " \n",
    "y_percent_error_test.std()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show scatter plot of error\n",
    "\n",
    "plt.scatter(y_te,y_pred_test)\n",
    "plt.plot(y_te,y_te,'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63feb48-359e-4b07-890b-dec1434fe889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9eafa-b8a3-4293-bbc7-f98c699ca13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988de47-e313-4bd9-98ff-161f5f77bdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de8bc2-56f6-46f2-9e6f-81e9197582f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad594b-9e9c-4e74-8970-937105519aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420b360-2347-4919-a3cf-066cfe16ba8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59196be-71c7-4592-befa-48c165839746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
